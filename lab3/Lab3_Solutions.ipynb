{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aamini/introtodeeplearning_labs/blob/lab3/lab3/Lab3_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WoXYKhfZMHiw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Laboratory 3: Reinforcement Learning\n",
        "\n",
        "FIXME: a short RL intro.  \n",
        "![alt text](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
        "\n",
        "## Why do we care about games? \n",
        "While the ultimate goal of reinforcement learning is to teach agents to act in the real, physical world, games provide a set of very useful properties that we also care about: \n",
        "\n",
        "1.   In many cases, games have perfectly describable enviornments. For example, all rules of chess can be formally written and programmed into a chess game simulator;\n",
        "2.   Massively parallelizable. Do not require running in the real world, therefore simultaneous environments can be run on large data clusters; \n",
        "3.   Fast prototyping of algorithms on simpler scenarios can speed up the development of algorithms that could eventually run in the real-world; and\n",
        "4.   ... Games are fun! \n",
        "\n",
        "In this lab, we focus on building a model-free reinforcement learning algorithm to master two different enviornments with varying complexity. \n",
        "\n",
        "1.   **Cartpole:   Balance a pole in an upright position by only moving your base left or right. Low-dimensional observation space.**\n",
        "2.   **Pong:   Beat a classical AI system designed at the game of Pong. High-dimensional observational space -- learning directly from raw pixels!  **\n"
      ]
    },
    {
      "metadata": {
        "id": "zmrHSiXKTXTY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1: Cartpole\n",
        "\n",
        "FIXME: have a preface here that breaks down what we'll do in this protion of the lab (i.e.: first define environment, then agent, then ...) since very different from prior labs would be good to have an image showing the pipeline / workflow for the lab. \n",
        "\n",
        "\n",
        "First we'll import TensorFlow, enable Eager execution, and also import some dependencies."
      ]
    },
    {
      "metadata": {
        "id": "xk5qeNPWCm00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "fd891145-9951-472d-8dfe-81cb9805a3cc"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "import time\n",
        "\n",
        "# Download the class repository\n",
        "! git clone https://github.com/aamini/introtodeeplearning_labs.git  > /dev/null 2>&1\n",
        "% cd introtodeeplearning_labs \n",
        "! git pull\n",
        "% cd .. \n",
        "\n",
        "import introtodeeplearning_labs as util"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/introtodeeplearning_labs\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/aamini/introtodeeplearning_labs\n",
            "   498c7c9..84529b5  2019       -> origin/2019\n",
            "Already up to date.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UT7YL8KBJIIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Define and inspect the environment\n",
        "\n",
        "FIXME: need a short text intro here about what is meant by the environment, gym.make() function call, the other relevant attributes / functions for the environment"
      ]
    },
    {
      "metadata": {
        "id": "quv9SC0iIYFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "2a6555ea-37a2-4946-92d3-91f0d754b3ac"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.seed(1) # reproducible, since RL has high variance"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1L]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "mhEITUcKK455",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FIXME: more background text on the observation space. Can include a schematic image of cart-pole control such as this one: https://danielpiedrahita.wordpress.com/portfolio/cart-pole-control/\n",
        "\n",
        "Observations:\n",
        "\n",
        "1. position of cart\n",
        "2. velocity of cart\n",
        "3. angle of pole\n",
        "4. rotation rate of pole\n",
        "\n",
        "We can confirm the size of the space by querying the observation space\n"
      ]
    },
    {
      "metadata": {
        "id": "UVJaEcbdIX82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "942e4ab2-568d-4ef1-ef34-a0ccce819bab"
      },
      "cell_type": "code",
      "source": [
        "print \"Enviornment has observation space = {}\".format(env.observation_space)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enviornment has observation space = Box(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZibGgjrALgPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FIXME: can also have a schematic imager her indicating the agents action space. \n",
        "\n",
        "At every time step, the agent can move either right or left. Again, we can confirm the size of the action space again by querying the environment"
      ]
    },
    {
      "metadata": {
        "id": "qc9SIPxBIXrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8372d77-58b7-46cf-d3cf-5c8934ee3974"
      },
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.n\n",
        "print \"Number of possible actions that the agent can choose from = {}\".format(n_actions)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of possible actions that the agent can choose from = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pPfHME8aRKkb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 Define the Agent\n",
        "\n",
        "Let's define our agent, which is simply a deep neural network which takes as input an observation of the enviornment and outputs the probability of taking each of the possible actions. \n",
        "\n",
        "FIXME: schematic/figure defintely helpful here, esp if did not include the cart pull example. \n"
      ]
    },
    {
      "metadata": {
        "id": "W-o_XK4oQ4eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cartpole_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "      tf.keras.layers.Dense(units=n_actions, activation=None)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "cartpole_model = create_cartpole_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5D5NSIYS2IW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the action function that executes a forward pass through the network and samples from the output. Take special note of the output activation of the model."
      ]
    },
    {
      "metadata": {
        "id": "E_vVZRr8Q4R_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def choose_action(model, observation):\n",
        "    \n",
        "  observation = observation.reshape([1, -1])\n",
        "  logits = model.predict(observation)\n",
        "\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()\n",
        "\n",
        "  action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0]\n",
        "\n",
        "  return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tR9uAWcTnkr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Create the agent's memory\n",
        "\n",
        "During training, the agent will need to remember all of its observations, actions so that once the episode ends, it can \"reinforce\" the good actions and punish the undesirable actions. Let's do this by defining a simple memory buffer that contains the FIXME : need to complete the sentence here. "
      ]
    },
    {
      "metadata": {
        "id": "8MM6JwXVQ4JG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "  def __init__(self): \n",
        "      self.clear()\n",
        "\n",
        "  def clear(self): \n",
        "      self.observations = []\n",
        "      self.actions = []\n",
        "      self.rewards = []\n",
        "\n",
        "  def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "      self.observations.append(new_observation)\n",
        "      self.actions.append(new_action)\n",
        "      self.rewards.append(new_reward)\n",
        "        \n",
        "memory = Memory()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4YhtPaUVj5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're almost ready to begin the learning algorithm for our agent! The final step is to compute the discounted rewards of our agent. Recall from lecture, we use reward discount to give more preference at getting rewards now rather than later in the future. The idea of discounting rewards is similar to discounting money in the case of interest and can be defined as: \n",
        "\n",
        "FIXME: put the equation for discounted rewards here -- structure the equation similar to the code so we can ask students to complete the code given the equations\n"
      ]
    },
    {
      "metadata": {
        "id": "5_Q2OFYtQ32X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "  x -= np.mean(x)\n",
        "  x -= np.std(x)\n",
        "  return x\n",
        "\n",
        "def discount_rewards(rewards, gamma=0.95): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QzbY-mjGYcmt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 Define the learning algorithm\n",
        "\n",
        "FIXME: preface with some general sentence about RL learnin and optimization.\n",
        "Start by defining the optimizer we want to use."
      ]
    },
    {
      "metadata": {
        "id": "m3u6xDNMY0zg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-LJwWqTZegG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now let's define the loss function. In this lab we are focusing on policy gradient methods which aim to **maximize** the likelihood of actions that result in large rewards. Equivalently, this means that we want to **minimize** the negative likelihood of these same actions. Like in supervised learning, we can use stochastic gradient descent methods to achieve this minimization. \n",
        "\n",
        "Since the log function is monotonically increasing, this means that minimizing negative **likelihood** is equivalent to minimizing negative **log-likelihood**.  Recall that we can easily compute the negative log-likelihood of an discrete action by evaluting its softmax cross entropy (https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) "
      ]
    },
    {
      "metadata": {
        "id": "fsgZ3IDCY_Zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_loss(logits, actions, rewards): \n",
        "  neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
        "  loss = tf.reduce_mean( neg_logprob * rewards )\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rr5vQ9fqbPpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's use the loss function to define a backpropogation step of our learning algorithm."
      ]
    },
    {
      "metadata": {
        "id": "_50ada7nbZ7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      # Forward propogate through the agent\n",
        "      observations = tf.convert_to_tensor(observations, dtype=tf.float32)\n",
        "      logits = model(observations)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "  # Backpropagation\n",
        "  grads = tape.gradient(loss, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XsjKXh6BcgjR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.5 Let the agent go and watch it learn from scratch!\n",
        "\n",
        "FIXME: sentence description of what is going on here! \"let tthe agent go\" means what exactly? i think needsw to be specified. \n"
      ]
    },
    {
      "metadata": {
        "id": "XmOzc2rrcn8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cartpole_model = create_cartpole_model()\n",
        "\n",
        "smoothed_reward = util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = util.PeriodicPlotter(sec=5, xlabel='Iterations', ylabel='Rewards')\n",
        "\n",
        "\n",
        "for i_episode in range(1000):\n",
        "\n",
        "  plotter.plot(smoothed_reward.get())\n",
        "\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "\n",
        "  while True:\n",
        "      action = choose_action(cartpole_model, observation)\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      memory.add_to_memory(observation, action, reward)\n",
        "\n",
        "      if done:\n",
        "          total_reward = sum(memory.rewards)\n",
        "          smoothed_reward.append( total_reward )\n",
        "\n",
        "          train_step(cartpole_model, \n",
        "                     optimizer, \n",
        "                     observations = np.vstack(memory.observations),\n",
        "                     actions = np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "          \n",
        "          memory.clear()\n",
        "          break\n",
        "\n",
        "      observation = next_observation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkcUtGF1VE-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.6 Save a video of the trained model while it is balancing the pole"
      ]
    },
    {
      "metadata": {
        "id": "M40RoTBxo3HD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f701269-bc0e-4b0b-fbbe-20ff9594a979"
      },
      "cell_type": "code",
      "source": [
        "def save_video_of_model(model, env_name, filename='agent.mp4'):  \n",
        "  import skvideo.io\n",
        "  from pyvirtualdisplay import Display\n",
        "  display = Display(visible=0, size=(40, 30))\n",
        "  display.start()\n",
        "\n",
        "  env = gym.make(env_name)\n",
        "  obs = env.reset()\n",
        "  shape = env.render(mode='rgb_array').shape[0:2]\n",
        "\n",
        "  out = skvideo.io.FFmpegWriter(filename)\n",
        "\n",
        "  done = False\n",
        "  while not done: \n",
        "      frame = env.render(mode='rgb_array')\n",
        "      out.writeFrame(frame)\n",
        "      \n",
        "      action = model(tf.convert_to_tensor(obs.reshape((1,-1)), tf.float32)).numpy().argmax()\n",
        "      obs, reward, done, info = env.step(action)\n",
        "  out.close()\n",
        "  print \"Successfully saved into {}!\".format(filename)\n",
        "\n",
        "save_video_of_model(cartpole_model, \"CartPole-v0\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully saved into agent.mp4!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvvqdwO7VV_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.7 Display the saved video\n"
      ]
    },
    {
      "metadata": {
        "id": "DBjhWQ0XwQ1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "711d2e99-2fe0-45a4-cc4d-1b426c452b50"
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "import io, base64\n",
        "video = io.open('./agent.mp4', 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "HTML(data='''\n",
        "<video controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "</video>'''.format(encoded.decode('ascii')))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video controls>\n",
              "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "</video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "CSbVNDpaVb3_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations, well done! How does the agent perform? Could you train it for shorter amounts of time and still perform well? Would training longer help even more? "
      ]
    },
    {
      "metadata": {
        "id": "Eu6Mqxc720ST",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2: Pong\n",
        "\n",
        "FIXME: need to rovide some more background here on why pong and cart pole are different, why we should care about each.. "
      ]
    },
    {
      "metadata": {
        "id": "srZ4YE29isuA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Define and inspect the environment"
      ]
    },
    {
      "metadata": {
        "id": "lbYHLr66i15n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fbcd702-5ed4-47d1-9320-bdd72de31e52"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pong-v0\")\n",
        "env.seed(1) # reproducible, since RL has high variance"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1L, 289714752L]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "52uZ2Xhyi-MW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Observations: \n",
        "\n",
        "1. RGB image of shape (210, 160, 3)\n",
        "\n",
        "We can again confirm the size of the observation space by query:"
      ]
    },
    {
      "metadata": {
        "id": "0yX4GWvxjnHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "470af331-48ab-4679-d6f4-7262ccc99e02"
      },
      "cell_type": "code",
      "source": [
        "print \"Enviornment has observation space = {}\".format(env.observation_space)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enviornment has observation space = Box(210, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuEC2TdSjx9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At every time step, the agent has six actions to choose from: noop, fire, move right, move left, fire right, and fire left.Let's confirm the size of the action space by querying the environment:"
      ]
    },
    {
      "metadata": {
        "id": "Iuy9oPc1kag3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f9a6692-2256-45f1-a825-6e654197173d"
      },
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.n\n",
        "print \"Number of possible actions that the agent can choose from = {}\".format(n_actions)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of possible actions that the agent can choose from = 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9-fghDRigUE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Define the Agent\n",
        "\n",
        "We'll define our agent again, but this time, we'll add convolutional layers to the network to increase the learning capacity of our network."
      ]
    },
    {
      "metadata": {
        "id": "IJiqbFYpgYRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_pong_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      # Define and reshape inputs\n",
        "      tf.keras.layers.InputLayer(input_shape=(80, 80, 1), dtype=tf.float32),\n",
        "      tf.keras.layers.Reshape((80, 80, 1)),\n",
        "      \n",
        "      # Convolutional layers\n",
        "      tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4), activation='relu', padding='same'),\n",
        "      tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2), activation='relu', padding='same'),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      \n",
        "      # Fully connected layer and output\n",
        "      tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "      tf.keras.layers.Dense(units=n_actions, activation=None)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "pong_model = create_pong_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yaeZ067olFiJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since we've already defined the action function, `choose_action(model, observation)`, we don't need to define it again. Instead, we'll be able to reuse it later on by passing in our new model we've just created, `pong_model`. "
      ]
    },
    {
      "metadata": {
        "id": "l0RvqOVkmc2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 Helper Functions"
      ]
    },
    {
      "metadata": {
        "id": "g4xtfog0mupM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've already implemented some functions in Part 1 (Cartpole), so we won't need to recreate them in this section. However, we might need to make some slight modifications. One such is resetting the reward to zero when a game ends. In Pong, we know a game has ended if the reward is +1 (we won!) or -1 (we lost unfortunately). Otherwise, we expect the reward at a timestep to be zero. Also note that we've increased gamma from 0.95 to 0.99, so the rate of decay will be even more rapid."
      ]
    },
    {
      "metadata": {
        "id": "iEZG2o50luLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, gamma=0.99): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      # NEW: Reset sum\n",
        "      if rewards[t] != 0:\n",
        "        R = 0\n",
        "      \n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HopLpb4IoOqA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we input an image into our network, we'll need to pre-process it by converting it into a 1D array of floating point numbers:"
      ]
    },
    {
      "metadata": {
        "id": "Drpkn38Goout",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pre_process(image):\n",
        "  I = image[35:195] # Crop\n",
        "  I = I[::2, ::2, 0] # Downsample width and height by a factor of 2\n",
        "  I[I == 144] = 0 # Remove background type 1\n",
        "  I[I == 109] = 0 # Remove background type 2\n",
        "  I[I != 0] = 1 # Set remaining elements (paddles, ball, etc.) to 1\n",
        "  return I.astype(np.float).ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-tP8_Bna6pgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FIXME: could we show an image now of the env before and after preprocessing to visualize the difference?"
      ]
    },
    {
      "metadata": {
        "id": "mRqcaDQ1pm3x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4: Training\n",
        "We've already defined our loss function with `compute_loss`, which is great! If we want to use a different learning rate, though, we can reinitialize the `optimizer`:"
      ]
    },
    {
      "metadata": {
        "id": "cIjRZ8JUqBLV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate=1e-4\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IL2lMbTDn6Z3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also implement a very simple variant of `plot_progress`. In Pong, rather than feeding our network one image at a time, it can actually improve performance to input the difference between two consecutive observations, which really gives us information about the movement between frames. We'll first pre-process the raw observation, `x`, and then we'll compute the difference with the image frame we saw one timestep before. We'll also increase the number of maximum iterations from 1000 to 10000, since we expect it to take many more iterations to learn a more complex game."
      ]
    },
    {
      "metadata": {
        "id": "xCwyQQrPnkZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pong_model = create_pong_model()\n",
        "MAX_ITERS = 10000\n",
        "\n",
        "smoothed_reward = util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = util.PeriodicPlotter(sec=5, xlabel='Iterations', ylabel='Rewards')\n",
        "\n",
        "for i_episode in range(MAX_ITERS):\n",
        "\n",
        "  plotter.plot(smoothed_reward.get())\n",
        "\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "  previous_frame = pre_process(observation)\n",
        "\n",
        "\n",
        "  while True:\n",
        "      # Pre-process image \n",
        "      current_frame = pre_process(observation)\n",
        "\n",
        "      obs_change = current_frame - previous_frame\n",
        "      \n",
        "      action = choose_action(pong_model, obs_change) # Use frame difference \n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      memory.add_to_memory(obs_change, action, reward) # Save frame difference\n",
        "\n",
        "      if done:\n",
        "          total_reward = sum(memory.rewards)\n",
        "          smoothed_reward.append( total_reward )\n",
        "          import pdb; pdb.set_trace()\n",
        "\n",
        "          train_step(pong_model, \n",
        "                     optimizer, \n",
        "                     observations = np.vstack(memory.observations), #FIXME: this is not running for me -- does it work for you? \n",
        "                     actions = np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "          \n",
        "          memory.clear()\n",
        "          break\n",
        "\n",
        "      observation = next_observation\n",
        "      previous_frame = current_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwXjQH-puH5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5: Save and display video of training"
      ]
    },
    {
      "metadata": {
        "id": "8LiEY5Y_ts-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now save the video of our model learning:"
      ]
    },
    {
      "metadata": {
        "id": "TvHXbkL0tR6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_video_of_model(pong_model, \"Pong-v0\", filename='pong_agent.mp4')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmIcylIzuWaL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And display the result:"
      ]
    },
    {
      "metadata": {
        "id": "qoOBQSrXt2Ib",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "import io, base64\n",
        "video = io.open('./agent2.mp4', 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "HTML(data='''\n",
        "<video controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "</video>'''.format(encoded.decode('ascii')))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}