{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aamini/introtodeeplearning_labs/blob/lab3/lab3/Lab3_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WoXYKhfZMHiw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Laboratory 3: Reinforcement Learning\n",
        "\n",
        "Reinforcement learning (RL) is a subset of machine learning which poses learning problems as interactions between agents and environments. It often assumes agents have no prior knowledge of the given world, so they must learn to navigate environments by optimizing some provided reward function. Within a world, an agent can take certain actions and receive feedback--in the form of positive or negative rewards--with respect to their decision. As such, an agent's feedback loop is somewhat akin to the manner in which a child might learn to distinguish between \"good\" and \"bad\" actions. In practical terms, our RL agent will interact with the environment by taking an action at each timestep, receiving a corresponding reward, and updating its state according to what it's \"learned\".  \n",
        "\n",
        "![alt text](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
        "\n",
        "## Why do we care about games? \n",
        "While the ultimate goal of reinforcement learning is to teach agents to act in the real, physical world, games provide a set of very useful properties that we also care about: \n",
        "\n",
        "1.   In many cases, games have perfectly describable enviornments. For example, all rules of chess can be formally written and programmed into a chess game simulator;\n",
        "2.   Massively parallelizable. Do not require running in the real world, therefore simultaneous environments can be run on large data clusters; \n",
        "3.   Fast prototyping of algorithms on simpler scenarios can speed up the development of algorithms that could eventually run in the real-world; and\n",
        "4.   ... Games are fun! \n",
        "\n",
        "In this lab, we focus on building a model-free reinforcement learning algorithm to master two different enviornments with varying complexity. \n",
        "\n",
        "1.   **Cartpole:   Balance a pole in an upright position by only moving your base left or right. Low-dimensional observation space.**\n",
        "2.   **Pong:   Beat a classical AI system designed at the game of Pong. High-dimensional observational space -- learning directly from raw pixels!  **\n"
      ]
    },
    {
      "metadata": {
        "id": "zmrHSiXKTXTY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1: Cartpole\n",
        "\n",
        "Since we're no longer dealing with a supervised learning task, let's take a step back and outline our approach to the problem. First, we'll need to create our environment and initialize our agent. Moreover, we'll need to provide our agent with some sort of mechanism for remembering action and reward history with its environment it to be able to learn from it... in other words, a memory bank. Then we'll need to define our learning algorithm, much like we've done in previous labs.\n",
        "\n",
        "\n",
        "First we'll import TensorFlow, enable Eager execution, and also import some dependencies."
      ]
    },
    {
      "metadata": {
        "id": "xk5qeNPWCm00",
        "colab_type": "code",
        "outputId": "c03ff29e-90b6-4aad-face-942bf62228df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "import time\n",
        "\n",
        "# Download the class repository\n",
        "! git clone https://github.com/aamini/introtodeeplearning_labs.git  > /dev/null 2>&1\n",
        "% cd introtodeeplearning_labs \n",
        "! git pull\n",
        "% cd .. \n",
        "\n",
        "import introtodeeplearning_labs as util"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/introtodeeplearning_labs\n",
            "Already up to date.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UT7YL8KBJIIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Define and inspect the environment\n",
        "\n",
        "In order to model our environment, we'll be using a toolkit developed by OpenAI, [OpenAI Gym](https://gym.openai.com/). It provides several pre-defined environments for training and testing reinforcement learning agents, including those for classic physics control tasks, Atari video games, and robotic simulations. To access the basic version of a control task, \"Cart Pole\", we can use `env = gym.make(\"CartPole-v0\")`. When we imported `gym`, we gained access to higher level functions in the package, including creating virtual worlds. Each environment has a specific identifier (for which you can read through [here](https://gym.openai.com/envs/#classic_control)) which is accessed by encasing the environment name with string quotes.\n",
        "One issue we might experience when developing RL algorithms is that many aspects of the larning process are inherently random: initializing game states, changes in the environment, and the agent's actions. As such, it can be helpful to set a random \"seed\" for one of these variables to ensure some level of reproducibility. Much like you might use `numpy.random.seed`, we can call the comparable function in gym, `seed`, with our defined environment to ensure the environment's random variables are initialized the same each time. "
      ]
    },
    {
      "metadata": {
        "id": "quv9SC0iIYFm",
        "colab_type": "code",
        "outputId": "171bc4a5-41bb-48f1-94e4-4805beba3ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.seed(1) # reproducible, since RL has high variance"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1L]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "mhEITUcKK455",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**CartPole Environment: ** \n",
        "\n",
        "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
        "\n",
        "<img width=\"400px\" src=\"https://danielpiedrahita.files.wordpress.com/2017/02/cart-pole.png\"></img>\n",
        "\n",
        "Observations:\n",
        "\n",
        "1. position of cart\n",
        "2. velocity of cart\n",
        "3. angle of pole\n",
        "4. rotation rate of pole\n",
        "\n",
        "We can confirm the size of the space by querying the observation space\n"
      ]
    },
    {
      "metadata": {
        "id": "UVJaEcbdIX82",
        "colab_type": "code",
        "outputId": "71c8901c-c2c7-4aa8-e714-14a528a26d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print \"Enviornment has observation space = {}\".format(env.observation_space)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enviornment has observation space = Box(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZibGgjrALgPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At every time step, the agent can move either right or left. Again, we can confirm the size of the action space again by querying the environment"
      ]
    },
    {
      "metadata": {
        "id": "qc9SIPxBIXrm",
        "colab_type": "code",
        "outputId": "468466ac-e37c-4a96-f8e9-689c72808030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.n\n",
        "print \"Number of possible actions that the agent can choose from = {}\".format(n_actions)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of possible actions that the agent can choose from = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pPfHME8aRKkb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 Define the Agent\n",
        "\n",
        "Let's define our agent, which is simply a deep neural network which takes as input an observation of the enviornment and outputs the probability of taking each of the possible actions. \n"
      ]
    },
    {
      "metadata": {
        "id": "W-o_XK4oQ4eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cartpole_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "      tf.keras.layers.Dense(units=n_actions, activation=None)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "cartpole_model = create_cartpole_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5D5NSIYS2IW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the action function that executes a forward pass through the network and samples from the output. Take special note of the output activation of the model."
      ]
    },
    {
      "metadata": {
        "id": "E_vVZRr8Q4R_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def choose_action(model, observation):\n",
        "    \n",
        "  observation = observation.reshape([1, -1])\n",
        "  logits = model.predict(observation)\n",
        "\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()\n",
        "\n",
        "  action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0]\n",
        "\n",
        "  return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tR9uAWcTnkr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Create the agent's memory\n",
        "\n",
        "During training, the agent will need to remember all of its observations, actions so that once the episode ends, it can \"reinforce\" the good actions and punish the undesirable actions. Let's do this by defining a simple memory buffer that contains the agent's observations, actions, and received rewards from a given episode. "
      ]
    },
    {
      "metadata": {
        "id": "8MM6JwXVQ4JG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "  def __init__(self): \n",
        "      self.clear()\n",
        "\n",
        "  def clear(self): \n",
        "      self.observations = []\n",
        "      self.actions = []\n",
        "      self.rewards = []\n",
        "\n",
        "  def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "      self.observations.append(new_observation)\n",
        "      self.actions.append(new_action)\n",
        "      self.rewards.append(new_reward)\n",
        "        \n",
        "memory = Memory()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4YhtPaUVj5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're almost ready to begin the learning algorithm for our agent! The final step is to compute the discounted rewards of our agent. Recall from lecture, we use reward discount to give more preference at getting rewards now rather than later in the future. The idea of discounting rewards is similar to discounting money in the case of interest and can be defined as: \n",
        "\n",
        "FIXME: put the equation for discounted rewards here -- structure the equation similar to the code so we can ask students to complete the code given the equations\n"
      ]
    },
    {
      "metadata": {
        "id": "5_Q2OFYtQ32X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "  x -= np.mean(x)\n",
        "  x -= np.std(x)\n",
        "  return x\n",
        "\n",
        "def discount_rewards(rewards, gamma=0.95): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QzbY-mjGYcmt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 Define the learning algorithm\n",
        "\n",
        "Now we can start to define the learing algorithm which will be used to reinforce good behaviors of the agent and discourage bad behaviours. Start by defining the optimizer we want to use."
      ]
    },
    {
      "metadata": {
        "id": "m3u6xDNMY0zg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-LJwWqTZegG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now let's define the loss function. In this lab we are focusing on policy gradient methods which aim to **maximize** the likelihood of actions that result in large rewards. Equivalently, this means that we want to **minimize** the negative likelihood of these same actions. Like in supervised learning, we can use stochastic gradient descent methods to achieve this minimization. \n",
        "\n",
        "Since the log function is monotonically increasing, this means that minimizing negative **likelihood** is equivalent to minimizing negative **log-likelihood**.  Recall that we can easily compute the negative log-likelihood of an discrete action by evaluting its softmax cross entropy (https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) "
      ]
    },
    {
      "metadata": {
        "id": "fsgZ3IDCY_Zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_loss(logits, actions, rewards): \n",
        "  neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
        "  loss = tf.reduce_mean( neg_logprob * rewards )\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rr5vQ9fqbPpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's use the loss function to define a backpropogation step of our learning algorithm."
      ]
    },
    {
      "metadata": {
        "id": "_50ada7nbZ7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      # Forward propogate through the agent\n",
        "      observations = tf.convert_to_tensor(observations, dtype=tf.float32)\n",
        "      logits = model(observations)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "  # Backpropagation\n",
        "  grads = tape.gradient(loss, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XsjKXh6BcgjR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.5 Let the agent go and watch it learn from scratch!\n",
        "\n",
        "Having had no prior knowledge of the environment, the agent will begin to learn how to balance the pole on the cart based only on the feedback received from the environment! Having defined how our agent can move, how it takes in new observations, and how it updates its state, we'll see how it gradually learns a policy of actions to optimize balancing the pole as long as possible.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XmOzc2rrcn8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cartpole_model = create_cartpole_model()\n",
        "\n",
        "smoothed_reward = util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = util.PeriodicPlotter(sec=5, xlabel='Iterations', ylabel='Rewards')\n",
        "\n",
        "\n",
        "for i_episode in range(1000):\n",
        "\n",
        "  plotter.plot(smoothed_reward.get())\n",
        "\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "\n",
        "  while True:\n",
        "      action = choose_action(cartpole_model, observation)\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      memory.add_to_memory(observation, action, reward)\n",
        "\n",
        "      if done:\n",
        "          total_reward = sum(memory.rewards)\n",
        "          smoothed_reward.append( total_reward )\n",
        "\n",
        "          train_step(cartpole_model, \n",
        "                     optimizer, \n",
        "                     observations = np.vstack(memory.observations),\n",
        "                     actions = np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "          \n",
        "          memory.clear()\n",
        "          break\n",
        "\n",
        "      observation = next_observation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkcUtGF1VE-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.6 Save a video of the trained model while it is balancing the pole"
      ]
    },
    {
      "metadata": {
        "id": "M40RoTBxo3HD",
        "colab_type": "code",
        "outputId": "56c8f4d4-171e-4c3b-d7b7-ac06056d30ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def save_video_of_model(model, env_name, filename='agent.mp4'):  \n",
        "  import skvideo.io\n",
        "  from pyvirtualdisplay import Display\n",
        "  display = Display(visible=0, size=(40, 30))\n",
        "  display.start()\n",
        "\n",
        "  env = gym.make(env_name)\n",
        "  obs = env.reset()\n",
        "  shape = env.render(mode='rgb_array').shape[0:2]\n",
        "\n",
        "  out = skvideo.io.FFmpegWriter(filename)\n",
        "\n",
        "  done = False\n",
        "  while not done: \n",
        "      frame = env.render(mode='rgb_array')\n",
        "      out.writeFrame(frame)\n",
        "      \n",
        "      action = model(tf.convert_to_tensor(obs.reshape((1,-1)), tf.float32)).numpy().argmax()\n",
        "      obs, reward, done, info = env.step(action)\n",
        "  out.close()\n",
        "  print \"Successfully saved into {}!\".format(filename)\n",
        "\n",
        "save_video_of_model(cartpole_model, \"CartPole-v0\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully saved into agent.mp4!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvvqdwO7VV_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.7 Display the saved video\n"
      ]
    },
    {
      "metadata": {
        "id": "DBjhWQ0XwQ1d",
        "colab_type": "code",
        "outputId": "2cda8ecc-d246-48c9-8002-147709202463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "import io, base64\n",
        "video = io.open('./agent.mp4', 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "HTML(data='''\n",
        "<video controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "</video>'''.format(encoded.decode('ascii')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video controls>\n",
              "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "</video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "CSbVNDpaVb3_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations, well done! How does the agent perform? Could you train it for shorter amounts of time and still perform well? Would training longer help even more? "
      ]
    },
    {
      "metadata": {
        "id": "Eu6Mqxc720ST",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2: Pong\n",
        "\n",
        "In Cart Pole, we dealt with an environment that was static--in other words, it didn't change over time. What happens if our environment is dynamic and unpredictable? Well that's exactly the case in Pong, since part of the environment is our opposing player. We don't know how our opponent will act or react to our actions, so the complexity of our problem increases. "
      ]
    },
    {
      "metadata": {
        "id": "srZ4YE29isuA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Define and inspect the environment"
      ]
    },
    {
      "metadata": {
        "id": "lbYHLr66i15n",
        "colab_type": "code",
        "outputId": "f25baa92-9016-4145-856f-84950a37d262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pong-v0\")\n",
        "env.seed(1) # reproducible, since RL has high variance"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1L, 289714752L]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "52uZ2Xhyi-MW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Observations: \n",
        "\n",
        "1. RGB image of shape (210, 160, 3)\n",
        "\n",
        "We can again confirm the size of the observation space by query:"
      ]
    },
    {
      "metadata": {
        "id": "0yX4GWvxjnHS",
        "colab_type": "code",
        "outputId": "0ef14313-9353-4a60-f49e-bc252faf2c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print \"Enviornment has observation space = {}\".format(env.observation_space)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enviornment has observation space = Box(210, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuEC2TdSjx9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At every time step, the agent has six actions to choose from: noop, fire, move right, move left, fire right, and fire left.Let's confirm the size of the action space by querying the environment:"
      ]
    },
    {
      "metadata": {
        "id": "Iuy9oPc1kag3",
        "colab_type": "code",
        "outputId": "2782895e-a154-46b5-b8da-0ccb00f718d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.n\n",
        "print \"Number of possible actions that the agent can choose from = {}\".format(n_actions)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of possible actions that the agent can choose from = 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9-fghDRigUE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Define the Agent\n",
        "\n",
        "We'll define our agent again, but this time, we'll add convolutional layers to the network to increase the learning capacity of our network."
      ]
    },
    {
      "metadata": {
        "id": "IJiqbFYpgYRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_pong_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      # Define and reshape inputs\n",
        "      tf.keras.layers.InputLayer(input_shape=(80, 80, 1), dtype=tf.float32),\n",
        "      tf.keras.layers.Reshape((80, 80, 1)),\n",
        "      \n",
        "      # Convolutional layers\n",
        "      tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4), activation='relu', padding='same'),\n",
        "      tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2), activation='relu', padding='same'),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      \n",
        "      # Fully connected layer and output\n",
        "      tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "      tf.keras.layers.Dense(units=n_actions, activation=None)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "pong_model = create_pong_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yaeZ067olFiJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since we've already defined the action function, `choose_action(model, observation)`, we don't need to define it again. Instead, we'll be able to reuse it later on by passing in our new model we've just created, `pong_model`. "
      ]
    },
    {
      "metadata": {
        "id": "l0RvqOVkmc2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 Helper Functions"
      ]
    },
    {
      "metadata": {
        "id": "g4xtfog0mupM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've already implemented some functions in Part 1 (Cartpole), so we won't need to recreate them in this section. However, we might need to make some slight modifications. One such is resetting the reward to zero when a game ends. In Pong, we know a game has ended if the reward is +1 (we won!) or -1 (we lost unfortunately). Otherwise, we expect the reward at a timestep to be zero. Also note that we've increased gamma from 0.95 to 0.99, so the rate of decay will be even more rapid."
      ]
    },
    {
      "metadata": {
        "id": "iEZG2o50luLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, gamma=0.99): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      # NEW: Reset sum\n",
        "      if rewards[t] != 0:\n",
        "        R = 0\n",
        "      \n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HopLpb4IoOqA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we input an image into our network, we'll need to pre-process it by converting it into a 1D array of floating point numbers:"
      ]
    },
    {
      "metadata": {
        "id": "Drpkn38Goout",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pre_process(image):\n",
        "  I = image[35:195] # Crop\n",
        "  I = I[::2, ::2, 0] # Downsample width and height by a factor of 2\n",
        "  I[I == 144] = 0 # Remove background type 1\n",
        "  I[I == 109] = 0 # Remove background type 2\n",
        "  I[I != 0] = 1 # Set remaining elements (paddles, ball, etc.) to 1\n",
        "  return I.astype(np.float).ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-tP8_Bna6pgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's use this function to visualize what an observation might look like before and after pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "no5IIYtFm8pI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "effb226e-8711-442e-84c9-187970a90758"
      },
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "for i in range(30):\n",
        "  observation, _,_,_ = env.step(0)\n",
        "observation_pp = pre_process(observation)\n",
        "\n",
        "f = plt.figure(figsize=(10,3))\n",
        "ax = f.add_subplot(121)\n",
        "ax2 = f.add_subplot(122)\n",
        "ax.imshow(observation); ax.grid('off');\n",
        "ax2.imshow(observation_pp.reshape((80,80))); ax2.grid('off'); plt.title('Preprocessed Observation')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Preprocessed Observation')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAADRCAYAAAAZmxFuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noHuwdK/YNuQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mRqcaDQ1pm3x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4: Training\n",
        "We've already defined our loss function with `compute_loss`, which is great! If we want to use a different learning rate, though, we can reinitialize the `optimizer`:"
      ]
    },
    {
      "metadata": {
        "id": "cIjRZ8JUqBLV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate=1e-4\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IL2lMbTDn6Z3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also implement a very simple variant of `plot_progress`. In Pong, rather than feeding our network one image at a time, it can actually improve performance to input the difference between two consecutive observations, which really gives us information about the movement between frames. We'll first pre-process the raw observation, `x`, and then we'll compute the difference with the image frame we saw one timestep before. We'll also increase the number of maximum iterations from 1000 to 10000, since we expect it to take many more iterations to learn a more complex game."
      ]
    },
    {
      "metadata": {
        "id": "xCwyQQrPnkZG",
        "colab_type": "code",
        "outputId": "048a994d-8fff-422c-9c7e-7f0504d464b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "pong_model = create_pong_model()\n",
        "MAX_ITERS = 10000\n",
        "\n",
        "smoothed_reward = util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = util.PeriodicPlotter(sec=5, xlabel='Iterations', ylabel='Rewards')\n",
        "\n",
        "for i_episode in range(MAX_ITERS):\n",
        "\n",
        "  plotter.plot(smoothed_reward.get())\n",
        "\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "  previous_frame = pre_process(observation)\n",
        "\n",
        "\n",
        "  while True:\n",
        "      # Pre-process image \n",
        "      current_frame = pre_process(observation)\n",
        "\n",
        "      obs_change = current_frame - previous_frame\n",
        "      \n",
        "      action = choose_action(pong_model, obs_change) # Use frame difference \n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      memory.add_to_memory(obs_change, action, reward) # Save frame difference\n",
        "\n",
        "      if done:\n",
        "          total_reward = sum(memory.rewards)\n",
        "          smoothed_reward.append( total_reward )\n",
        "\n",
        "          train_step(pong_model, \n",
        "                     optimizer, \n",
        "                     observations = np.vstack(memory.observations), #FIXME: this is not running for me -- does it work for you? \n",
        "                     actions = np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "          \n",
        "          memory.clear()\n",
        "          break\n",
        "\n",
        "      observation = next_observation\n",
        "      previous_frame = current_frame"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-895e6a6d12c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m           train_step(pong_model, \n\u001b[1;32m     31\u001b[0m                      \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                      \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#FIXME: this is not running for me -- does it work for you?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                      \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                      discounted_rewards = discount_rewards(memory.rewards))\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "nwXjQH-puH5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5: Save and display video of training"
      ]
    },
    {
      "metadata": {
        "id": "8LiEY5Y_ts-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now save the video of our model learning:"
      ]
    },
    {
      "metadata": {
        "id": "TvHXbkL0tR6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_video_of_model(pong_model, \"Pong-v0\", filename='pong_agent.mp4')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmIcylIzuWaL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And display the result:"
      ]
    },
    {
      "metadata": {
        "id": "qoOBQSrXt2Ib",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "import io, base64\n",
        "video = io.open('./agent2.mp4', 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "HTML(data='''\n",
        "<video controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "</video>'''.format(encoded.decode('ascii')))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}